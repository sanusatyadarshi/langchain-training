#!/bin/bash

echo "ğŸš€ Starting LangChain Workshop Environment..."

# Create checkpoint directory
mkdir -p /root

# Check API configuration
echo "ğŸ” Checking API configuration..."
if [ -n "$OPENAI_API_KEY" ] && [ -n "$OPENAI_API_BASE" ] && [ "$USE_REAL_API" = "true" ]; then
    echo "ğŸ”‘ API credentials detected - testing connectivity..."

    # Test API connectivity
    python3 -c "
import os
import requests
import json

try:
    api_base = os.environ.get('OPENAI_API_BASE', '').strip().strip('\"').rstrip('/')
    api_key = os.environ.get('OPENAI_API_KEY', '').strip().strip('\"')

    if not api_base or not api_key:
        print('âŒ Missing API configuration')
        exit(1)

    # Test with a simple request
    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }

    # Try to get models list or make a simple completion
    response = requests.get(f'{api_base}/models', headers=headers, timeout=10)

    if response.status_code == 200:
        print('âœ… API connectivity successful!')
        try:
            models = response.json()
            if 'data' in models and len(models['data']) > 0:
                print(f'ğŸ“‹ Available models: {len(models[\"data\"])} models found')
            else:
                print('ğŸ“‹ API connected but no models listed')
        except:
            print('ğŸ“‹ API connected (models endpoint may not be available)')
    else:
        print(f'âš ï¸  API responded with status {response.status_code}')
        print('   Workshop will continue but API calls may fail')

except requests.exceptions.ConnectTimeout:
    print('âŒ API connection timeout - check your OPENAI_API_BASE')
    print('   Workshop will continue in demo mode')
except requests.exceptions.ConnectionError:
    print('âŒ Cannot connect to API - check your network and OPENAI_API_BASE')
    print('   Workshop will continue in demo mode')
except Exception as e:
    print(f'âš ï¸  API test failed: {str(e)}')
    print('   Workshop will continue but API calls may fail')
"
else
    echo "ğŸ“š Running in demo mode (no API credentials provided)"
    echo "   To use real AI models, provide .env file with your LiteLLM credentials"
fi

# Pre-download sentence-transformers model to speed up workshop
echo "ğŸ“¥ Pre-downloading embedding model..."
python3 -c "
from langchain_huggingface import HuggingFaceEmbeddings
print('Initializing embeddings...')
embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
print('âœ… Embeddings ready!')
"

# Create welcome message
echo "
ğŸ‰ Welcome to the LangChain Workshop!

ğŸ“š Available Notebooks:
   â€¢ Task1_Setup.ipynb - Environment verification & API testing
   â€¢ Task2_Prompt_Templates.ipynb - Master prompt templates
   â€¢ Task3_Multiple_LLMs.ipynb - Connect to LLMs
   â€¢ Task4_LCEL_Pipelines.ipynb - Build LCEL chains
   â€¢ Task5_Memory_Systems.ipynb - Add conversational memory
   â€¢ Task6_RAG_System.ipynb - Create RAG systems
   â€¢ Task7_AI_Assistant.ipynb - Launch complete AI assistant

ğŸŒ Access Points:
   â€¢ Jupyter Lab: http://localhost:8888
   â€¢ Gradio App (Task 7): http://localhost:7860

ğŸ“‚ Workshop Files:
   â€¢ All Python files are pre-loaded in /workshop/task{1-7}/ directories
   â€¢ .env.example file available for API configuration
   â€¢ Sample documents in /workshop/data/ directory

ğŸ”§ Setup Options:
   â€¢ Demo Mode: Workshop works without API keys (current mode)
   â€¢ Production Mode: Use real AI models with your LiteLLM credentials

ğŸ’¡ To use real AI models:
   1. Copy .env.example to .env
   2. Update with your LiteLLM credentials
   3. Restart with: docker run --env-file .env -p 8888:8888 -p 7860:7860 langchain-workshop

Happy Learning! ğŸš€
"

# Start Jupyter Lab
echo "ğŸ”§ Starting Jupyter Lab on port 8888..."
exec jupyter lab --allow-root --ip=0.0.0.0 --port=8888 --no-browser --notebook-dir=/workshop