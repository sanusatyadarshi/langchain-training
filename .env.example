# LangChain Workshop - Environment Configuration
# Copy this file to .env and update with your actual LiteLLM credentials
#
# Usage:
#   1. Copy this file: cp .env.example .env
#   2. Update the values below with your actual credentials
#   3. Run workshop: docker run --env-file .env -p 8888:8888 -p 7860:7860 langchain-workshop

# =============================================================================
# LiteLLM Configuration (Required for real AI models)
# =============================================================================

# Your LiteLLM proxy endpoint (replace with your actual proxy URL)
OPENAI_API_BASE=https://your-litellm-proxy.com/v1

# Your LiteLLM authentication token (replace with your actual token)
OPENAI_API_KEY=your_litellm_auth_token_here

# =============================================================================
# Model Configuration
# =============================================================================

# Default model for most tasks (examples: gpt-4, gpt-3.5-turbo, claude-3-sonnet)
DEFAULT_MODEL=gpt-4

# Fast model for quick responses (examples: gpt-3.5-turbo, gpt-4o-mini)
FAST_MODEL=gpt-3.5-turbo

# Coding-focused model (examples: gpt-4, claude-3-sonnet, deepseek-coder)
CODING_MODEL=gpt-4

# Creative model for Task 4 examples (examples: gpt-4, claude-3-sonnet)
CREATIVE_MODEL=gpt-4

# =============================================================================
# Optional: Direct Provider API Keys (if not using LiteLLM proxy)
# =============================================================================

# Uncomment and set these if you want to use direct provider APIs instead of LiteLLM
# ANTHROPIC_API_KEY=your_anthropic_key_here
# GOOGLE_API_KEY=your_google_key_here
# COHERE_API_KEY=your_cohere_key_here

# =============================================================================
# Workshop Configuration
# =============================================================================

# Enable/disable real API calls (true/false)
# Set to false for demo mode without API calls
USE_REAL_API=true

# Timeout for API calls in seconds
API_TIMEOUT=30

# Maximum tokens for responses
MAX_TOKENS=1000

# =============================================================================
# Advanced Configuration
# =============================================================================

# Custom headers for your LiteLLM proxy (if required)
# CUSTOM_HEADERS={"X-Custom-Header": "value"}

# Rate limiting (requests per minute) - optional
# RATE_LIMIT=60

# Debug mode - shows detailed API request/response logs
DEBUG_MODE=false

# =============================================================================
# Example Configurations for Popular LiteLLM Setups
# =============================================================================

# Example 1: Local LiteLLM proxy
# OPENAI_API_BASE=http://localhost:4000/v1
# OPENAI_API_KEY=your_local_token

# Example 2: Hosted LiteLLM proxy
# OPENAI_API_BASE=https://api.your-domain.com/v1
# OPENAI_API_KEY=your_hosted_token

# Example 3: LiteLLM Cloud
# OPENAI_API_BASE=https://proxy.litellm.ai/v1
# OPENAI_API_KEY=your_litellm_cloud_token

# =============================================================================
# Instructions
# =============================================================================
#
# 1. Replace the placeholder values above with your actual credentials
# 2. Save this file as .env (without .example extension)
# 3. Run the workshop with: docker run --env-file .env -p 8888:8888 -p 7860:7860 langchain-workshop
#
# For demo mode (no API calls):
# - Set USE_REAL_API=false or simply don't create a .env file
#
# Troubleshooting:
# - If API calls fail, check your OPENAI_API_BASE and OPENAI_API_KEY
# - Set DEBUG_MODE=true to see detailed request logs
# - Check the LiteLLM documentation: https://docs.litellm.ai/
#