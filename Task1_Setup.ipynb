{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Setup - LangChain Environment\n",
    "\n",
    "Welcome to the LangChain Workshop! In this task, we'll verify that your environment is properly set up.\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.11+\n",
    "- All dependencies pre-installed in this Docker environment\n",
    "- Proxy provides OpenAI-compatible endpoints for all model IDs\n",
    "\n",
    "## What's Already Installed\n",
    "This Docker environment comes with all necessary packages pre-installed:\n",
    "- LangChain core packages\n",
    "- LLM providers (OpenAI, Anthropic, Google)\n",
    "- Vector stores and embeddings\n",
    "- UI libraries (Gradio)\n",
    "- All utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify LangChain Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test core LangChain imports\n",
    "try:\n",
    "    import langchain\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings\n",
    "    import gradio as gr\n",
    "    \n",
    "    print(\"✅ All core packages imported successfully!\")\n",
    "    print(f\"LangChain version: {langchain.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Basic LangChain Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt template creation\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Tell me about {topic}\"\n",
    ")\n",
    "\n",
    "formatted_prompt = template.format(topic=\"LangChain\")\n",
    "print(\"✅ Prompt template working:\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: API Configuration & Testing\n\nCheck your LiteLLM configuration and test API connectivity."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom workshop_config import config\n\n# Display current API configuration\nconfig.print_status()\n\n# Test API connectivity if configured\nif config.is_api_configured:\n    print(\"🚀 Testing API connectivity...\")\n    \n    # Test with a simple model call\n    model = config.get_model(\"default\", temperature=0)\n    if model:\n        try:\n            response = model.invoke(\"Hello! Please respond with 'API test successful'\")\n            print(f\"✅ API Test Result: {response.content}\")\n        except Exception as e:\n            print(f\"❌ API Test Failed: {e}\")\n            print(\"💡 Check your .env file configuration\")\n    else:\n        print(\"❌ Failed to create model instance\")\nelse:\n    print(\"📚 Demo mode active\")\n    print(\"💡 To use real AI models:\")\n    print(\"   1. Copy .env.example to .env\")\n    print(\"   2. Update with your LiteLLM credentials\")\n    print(\"   3. Restart container with --env-file .env\")\n\n# Test basic model initialization (demo mode)\ntry:\n    from langchain_openai import ChatOpenAI\n    demo_model = ChatOpenAI(\n        model=\"gpt-4\",\n        temperature=0\n    )\n    print(\"✅ Model class initialization successful\")\nexcept Exception as e:\n    print(f\"❌ Model class initialization failed: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Environment Configuration Summary\n\nView your complete workshop setup and next steps."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create checkpoint file to indicate setup is complete\nimport os\ncheckpoint_dir = \"/root\"\nos.makedirs(checkpoint_dir, exist_ok=True)\n\nwith open(f'{checkpoint_dir}/langchain-ready.txt', 'w') as f:\n    f.write(\"LANGCHAIN_INSTALLED\")\n\nprint(\"✅ Environment setup complete!\")\nprint(\"📂 Checkpoint file created: /root/langchain-ready.txt\")\nprint()\n\n# Display configuration summary\nprint(\"📋 Workshop Configuration Summary:\")\nprint(\"   ✅ Python 3.11+ environment\")\nprint(\"   ✅ All LangChain packages installed\")\nprint(\"   ✅ Vector stores and embeddings ready\")\nprint(\"   ✅ Jupyter Lab interface active\")\n\napi_status = \"✅ Real AI models\" if config.is_api_configured else \"📚 Demo mode\"\nprint(f\"   {api_status}\")\n\nprint()\nprint(\"🎯 Workshop Mode:\")\nif config.is_api_configured:\n    print(\"   🚀 PRODUCTION MODE - Using real AI models\")\n    print(f\"   📡 Connected to: {config.api_base}\")\n    print(f\"   🤖 Default model: {config.default_model}\")\nelse:\n    print(\"   📚 DEMO MODE - Using example responses\")\n    print(\"   💡 Configure .env file to use real AI models\")\n\nprint()\nprint(\"📁 Available Files:\")\nprint(\"   📄 .env.example - Template for your API configuration\")\nprint(\"   🐍 workshop_config.py - Centralized configuration utility\")\nprint(\"   📒 Task notebooks 1-7 - Progressive learning modules\")\nprint(\"   📂 task1-7/ directories - All Python examples ready to run\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What's Next?\n\nYour workshop environment is ready! Here's your learning path:\n\n### 🎓 Workshop Tasks\n- **Task 2**: Master Prompt Templates (basic, chat, few-shot, advanced)\n- **Task 3**: Connect to Multiple LLMs\n- **Task 4**: Build LCEL Pipelines  \n- **Task 5**: Add Memory to Conversations\n- **Task 6**: Create a RAG System\n- **Task 7**: Launch Your Complete AI Assistant\n\n### 🚀 Using Real AI Models\n\n**Current Status**: Demo mode (using example responses)\n\n**To enable real AI models**:\n1. **Copy template**: `cp .env.example .env`\n2. **Edit .env file** with your LiteLLM credentials:\n   ```bash\n   OPENAI_API_BASE=https://your-litellm-proxy.com/v1\n   OPENAI_API_KEY=your_auth_token\n   USE_REAL_API=true\n   ```\n3. **Restart container**:\n   ```bash\n   docker run --env-file .env -p 8888:8888 -p 7860:7860 langchain-workshop\n   ```\n\n### 📂 Workshop Structure\n```\n/workshop/\n├── .env.example              # API configuration template\n├── workshop_config.py        # Configuration utility\n├── Task1_Setup.ipynb         # Current notebook\n├── Task2_Prompt_Templates.ipynb\n├── Task3_Multiple_LLMs.ipynb\n├── Task4_LCEL_Pipelines.ipynb  \n├── Task5_Memory_Systems.ipynb\n├── Task6_RAG_System.ipynb\n├── Task7_AI_Assistant.ipynb\n├── task1/ to task7/          # Python examples\n└── data/                     # Sample documents\n```\n\n### 🎯 Quick Start Options\n\n**Option 1: Demo Mode (Current)**\n- Continue with example responses\n- Perfect for learning LangChain concepts\n- No API configuration needed\n\n**Option 2: Production Mode**  \n- Use your actual LiteLLM models\n- Get real AI responses\n- See production patterns\n\n**Ready to start?** → Open `Task2_Prompt_Templates.ipynb`"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}