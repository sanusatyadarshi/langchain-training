{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Setup - LangChain Environment\n",
    "\n",
    "Welcome to the LangChain Workshop! In this task, we'll verify that your environment is properly set up.\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.11+\n",
    "- All dependencies pre-installed in this Docker environment\n",
    "- Proxy provides OpenAI-compatible endpoints for all model IDs\n",
    "\n",
    "## What's Already Installed\n",
    "This Docker environment comes with all necessary packages pre-installed:\n",
    "- LangChain core packages\n",
    "- LLM providers (OpenAI, Anthropic, Google)\n",
    "- Vector stores and embeddings\n",
    "- UI libraries (Gradio)\n",
    "- All utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify LangChain Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test core LangChain imports\n",
    "try:\n",
    "    import langchain\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings\n",
    "    import gradio as gr\n",
    "    \n",
    "    print(\"âœ… All core packages imported successfully!\")\n",
    "    print(f\"LangChain version: {langchain.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Basic LangChain Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt template creation\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Tell me about {topic}\"\n",
    ")\n",
    "\n",
    "formatted_prompt = template.format(topic=\"LangChain\")\n",
    "print(\"âœ… Prompt template working:\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: API Configuration & Testing\n\nCheck your LiteLLM configuration and test API connectivity."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom workshop_config import config\n\n# Display current API configuration\nconfig.print_status()\n\n# Test API connectivity if configured\nif config.is_api_configured:\n    print(\"ğŸš€ Testing API connectivity...\")\n    \n    # Test with a simple model call\n    model = config.get_model(\"default\", temperature=0)\n    if model:\n        try:\n            response = model.invoke(\"Hello! Please respond with 'API test successful'\")\n            print(f\"âœ… API Test Result: {response.content}\")\n        except Exception as e:\n            print(f\"âŒ API Test Failed: {e}\")\n            print(\"ğŸ’¡ Check your .env file configuration\")\n    else:\n        print(\"âŒ Failed to create model instance\")\nelse:\n    print(\"ğŸ“š Demo mode active\")\n    print(\"ğŸ’¡ To use real AI models:\")\n    print(\"   1. Copy .env.example to .env\")\n    print(\"   2. Update with your LiteLLM credentials\")\n    print(\"   3. Restart container with --env-file .env\")\n\n# Test basic model initialization (demo mode)\ntry:\n    from langchain_openai import ChatOpenAI\n    demo_model = ChatOpenAI(\n        model=\"gpt-4\",\n        temperature=0\n    )\n    print(\"âœ… Model class initialization successful\")\nexcept Exception as e:\n    print(f\"âŒ Model class initialization failed: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Environment Configuration Summary\n\nView your complete workshop setup and next steps."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create checkpoint file to indicate setup is complete\nimport os\ncheckpoint_dir = \"/root\"\nos.makedirs(checkpoint_dir, exist_ok=True)\n\nwith open(f'{checkpoint_dir}/langchain-ready.txt', 'w') as f:\n    f.write(\"LANGCHAIN_INSTALLED\")\n\nprint(\"âœ… Environment setup complete!\")\nprint(\"ğŸ“‚ Checkpoint file created: /root/langchain-ready.txt\")\nprint()\n\n# Display configuration summary\nprint(\"ğŸ“‹ Workshop Configuration Summary:\")\nprint(\"   âœ… Python 3.11+ environment\")\nprint(\"   âœ… All LangChain packages installed\")\nprint(\"   âœ… Vector stores and embeddings ready\")\nprint(\"   âœ… Jupyter Lab interface active\")\n\napi_status = \"âœ… Real AI models\" if config.is_api_configured else \"ğŸ“š Demo mode\"\nprint(f\"   {api_status}\")\n\nprint()\nprint(\"ğŸ¯ Workshop Mode:\")\nif config.is_api_configured:\n    print(\"   ğŸš€ PRODUCTION MODE - Using real AI models\")\n    print(f\"   ğŸ“¡ Connected to: {config.api_base}\")\n    print(f\"   ğŸ¤– Default model: {config.default_model}\")\nelse:\n    print(\"   ğŸ“š DEMO MODE - Using example responses\")\n    print(\"   ğŸ’¡ Configure .env file to use real AI models\")\n\nprint()\nprint(\"ğŸ“ Available Files:\")\nprint(\"   ğŸ“„ .env.example - Template for your API configuration\")\nprint(\"   ğŸ workshop_config.py - Centralized configuration utility\")\nprint(\"   ğŸ“’ Task notebooks 1-7 - Progressive learning modules\")\nprint(\"   ğŸ“‚ task1-7/ directories - All Python examples ready to run\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What's Next?\n\nYour workshop environment is ready! Here's your learning path:\n\n### ğŸ“ Workshop Tasks\n- **Task 2**: Master Prompt Templates (basic, chat, few-shot, advanced)\n- **Task 3**: Connect to Multiple LLMs\n- **Task 4**: Build LCEL Pipelines  \n- **Task 5**: Add Memory to Conversations\n- **Task 6**: Create a RAG System\n- **Task 7**: Launch Your Complete AI Assistant\n\n### ğŸš€ Using Real AI Models\n\n**Current Status**: Demo mode (using example responses)\n\n**To enable real AI models**:\n1. **Copy template**: `cp .env.example .env`\n2. **Edit .env file** with your LiteLLM credentials:\n   ```bash\n   OPENAI_API_BASE=https://your-litellm-proxy.com/v1\n   OPENAI_API_KEY=your_auth_token\n   USE_REAL_API=true\n   ```\n3. **Restart container**:\n   ```bash\n   docker run --env-file .env -p 8888:8888 -p 7860:7860 langchain-workshop\n   ```\n\n### ğŸ“‚ Workshop Structure\n```\n/workshop/\nâ”œâ”€â”€ .env.example              # API configuration template\nâ”œâ”€â”€ workshop_config.py        # Configuration utility\nâ”œâ”€â”€ Task1_Setup.ipynb         # Current notebook\nâ”œâ”€â”€ Task2_Prompt_Templates.ipynb\nâ”œâ”€â”€ Task3_Multiple_LLMs.ipynb\nâ”œâ”€â”€ Task4_LCEL_Pipelines.ipynb  \nâ”œâ”€â”€ Task5_Memory_Systems.ipynb\nâ”œâ”€â”€ Task6_RAG_System.ipynb\nâ”œâ”€â”€ Task7_AI_Assistant.ipynb\nâ”œâ”€â”€ task1/ to task7/          # Python examples\nâ””â”€â”€ data/                     # Sample documents\n```\n\n### ğŸ¯ Quick Start Options\n\n**Option 1: Demo Mode (Current)**\n- Continue with example responses\n- Perfect for learning LangChain concepts\n- No API configuration needed\n\n**Option 2: Production Mode**  \n- Use your actual LiteLLM models\n- Get real AI responses\n- See production patterns\n\n**Ready to start?** â†’ Open `Task2_Prompt_Templates.ipynb`"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}